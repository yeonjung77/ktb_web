import json
import os
import urllib.error
import urllib.request
from collections import defaultdict
from typing import List, Dict, Any

from dotenv import load_dotenv
from fastapi import HTTPException, status

from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


load_dotenv()

# ë¡œì»¬ LLM(Ollama) ì„¤ì •
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
SOF_LLM_MODEL = os.getenv("SOF_LLM_MODEL", "llama3")


def _call_ollama_chat(messages: List[Dict[str, str]]) -> str:
    """
    Ollama /api/chat ì—”ë“œí¬ì¸íŠ¸ë¡œ ìš”ì²­ì„ ë³´ë‚´ëŠ” í—¬í¼.
    - Ollama ì•±ì´ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨
    - `ollama pull llama3` ë“±ìœ¼ë¡œ SOF_LLM_MODELì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì´ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•¨
    """
    url = f"{OLLAMA_HOST.rstrip('/')}/api/chat"
    payload = {
        "model": SOF_LLM_MODEL,
        "messages": messages,
        "stream": False,
        "options": {"temperature": 0.1, "top_p": 0.9},
    }

    data = json.dumps(payload).encode("utf-8")
    request = urllib.request.Request(
        url, data=data, headers={"Content-Type": "application/json"}
    )

    try:
        with urllib.request.urlopen(request, timeout=60) as response:
            body = response.read().decode("utf-8")
    except urllib.error.URLError as exc:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=(
                "íŒ¨ì…˜ ë¦¬ì„œì¹˜ìš© LLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ì™€ SOF_LLM_MODELì— ì§€ì •ëœ ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
            ),
        ) from exc
    except Exception as exc:  # pragma: no cover - ë°©ì–´ì  ì½”ë“œ
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="íŒ¨ì…˜ ë¦¬ì„œì¹˜ìš© LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
        ) from exc

    try:
        parsed = json.loads(body)
    except json.JSONDecodeError as exc:
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail="íŒ¨ì…˜ ë¦¬ì„œì¹˜ìš© LLM ì‘ë‹µì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        ) from exc

    # Ollama /api/chat ì‘ë‹µ í˜•ì‹: { ..., "message": {"role": "assistant", "content": "..."} }
    response_text = parsed.get("message", {}).get("content", "").strip()
    if not response_text:
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail="íŒ¨ì…˜ ë¦¬ì„œì¹˜ìš© LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.",
        )

    return response_text


_vectorstore: FAISS | None = None
_bm25_retriever: BM25Retriever | None = None
_by_year_chapter: Dict[Any, Any] | None = None
_by_chapter: Dict[Any, Any] | None = None

CHAPTER_LABELS = ["Global Economy", "Consumer Shifts", "Fashion System"]


def get_vectorstore() -> FAISS:
    global _vectorstore
    if _vectorstore is not None:
        return _vectorstore

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    base_dir = os.path.dirname(__file__)
    faiss_dir = os.path.join(base_dir, "faiss_index")

    if not os.path.isdir(faiss_dir):
        raise HTTPException(
            status_code=500,
            detail="faiss_index ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ktb_web4/faiss_index ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
        )

    _vectorstore = FAISS.load_local(
        faiss_dir, embeddings, allow_dangerous_deserialization=True
    )
    return _vectorstore


def get_bm25_retriever() -> BM25Retriever:
    global _bm25_retriever
    if _bm25_retriever is not None:
      return _bm25_retriever

    vs = get_vectorstore()
    all_docs = list(vs.docstore._dict.values())
    _bm25_retriever = BM25Retriever.from_documents(all_docs, k=50)
    return _bm25_retriever


def get_grouped_docs():
    global _by_year_chapter, _by_chapter
    if _by_year_chapter is not None and _by_chapter is not None:
        return _by_year_chapter, _by_chapter

    vs = get_vectorstore()
    all_docs = list(vs.docstore._dict.values())

    by_year_chapter = defaultdict(list)
    by_chapter = defaultdict(list)

    for d in all_docs:
        year = d.metadata.get("year")
        chapter = d.metadata.get("chapter")
        by_year_chapter[(year, chapter)].append(d)
        by_chapter[chapter].append(d)

    _by_year_chapter, _by_chapter = by_year_chapter, by_chapter
    return _by_year_chapter, _by_chapter


def hybrid_search(
    query: str,
    semantic_k: int = 30,
    keyword_k: int = 30,
    combined_k: int = 12,
    chapter_filter: str | None = None,
    region_filter: str | None = None,
):
    vs = get_vectorstore()
    bm25 = get_bm25_retriever()

    semantic_docs = vs.similarity_search(query, k=semantic_k)
    keyword_docs = bm25.invoke(query)[:keyword_k]

    def make_key(doc):
        return (
            doc.metadata.get("source"),
            doc.metadata.get("page"),
            doc.page_content,
        )

    scores: dict = {}
    n_sem = len(semantic_docs) or 1
    n_kw = len(keyword_docs) or 1

    for rank, doc in enumerate(semantic_docs):
        key = make_key(doc)
        sem_score = (n_sem - rank) / n_sem
        prev_sem, prev_kw, prev_doc = scores.get(key, (0.0, 0.0, doc))
        scores[key] = (max(prev_sem, sem_score), prev_kw, doc)

    for rank, doc in enumerate(keyword_docs):
        key = make_key(doc)
        kw_score = (n_kw - rank) / n_kw
        prev_sem, prev_kw, prev_doc = scores.get(key, (0.0, 0.0, doc))
        scores[key] = (prev_sem, max(prev_kw, kw_score), doc)

    alpha = 0.6
    scored_docs = []
    for sem_score, kw_score, doc in scores.values():
        final_score = alpha * sem_score + (1 - alpha) * kw_score

        if chapter_filter and doc.metadata.get("chapter") != chapter_filter:
            continue
        if region_filter and doc.metadata.get("region") != region_filter:
            continue

        scored_docs.append((final_score, doc))

    if not scored_docs:
        scored_docs = [
            (alpha * ((n_sem - i) / n_sem), d) for i, d in enumerate(semantic_docs)
        ]

    scored_docs.sort(key=lambda x: x[0], reverse=True)
    return [d for _, d in scored_docs[:combined_k]]


def format_docs(docs) -> str:
    processed = []
    for d in docs:
        src = os.path.basename(d.metadata.get("source", ""))
        page = d.metadata.get("page", "?")
        year = d.metadata.get("year", "")
        chapter = d.metadata.get("chapter", "")
        region = d.metadata.get("region", "")
        if region:
            header = f"[{year} / {chapter} / {region} / {src} p.{page}]"
        else:
            header = f"[{year} / {chapter} / {src} p.{page}]"
        processed.append(header + "\n" + d.page_content)
    return "\n\n".join(processed)


QA_SYSTEM_PROMPT = (
    "You are a professional Fashion MD Research Assistant.\n"
    "Use ONLY the content from McKinsey & BoF 'State of Fashion' (2021â€“2025).\n"
    "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤. ë¬¸ì¥Â·ë‹¨ë½ ì „ì²´ë¥¼ ì˜ì–´ë¡œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "í•„ìš”í•œ í•µì‹¬ ìš©ì–´(ì˜ˆ: ì§€í‘œ ì´ë¦„, ëª¨ë¸ëª… ë“±)ë§Œ ê´„í˜¸ ì•ˆì— ì˜ì–´ë¡œ ë³‘ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
    "ì§ˆë¬¸ì´ ì˜ì–´ë¡œ ë“¤ì–´ì™€ë„, ë‹µë³€ì€ í•­ìƒ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
    "ì°¸ê³  ë¬¸ì„œì— ëª…ì‹œì ìœ¼ë¡œ ì—†ëŠ” ìˆ˜ì¹˜, ì—°ë„, íšŒì‚¬ ì´ë¦„, ì„¸ë¶€ ì‚¬ë¡€ëŠ” ì¶”ì¸¡í•´ì„œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.\n"
    "ì°¸ê³  ë¬¸ì„œë§Œìœ¼ë¡œ ì¶©ë¶„í•œ ê·¼ê±°ê°€ ì—†ìœ¼ë©´, 'í•´ë‹¹ ë‚´ìš©ì€ State of Fashion ë¦¬í¬íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”."
)

REPORT_SYSTEM_PROMPT = (
    "You are a senior fashion strategy consultant.\n"
    "Below is a conversation between a Fashion MD and an AI research assistant\n"
    "about insights from McKinsey & BoF 'State of Fashion' (2021â€“2025).\n"
    "Use ONLY information that can be reasonably grounded in this conversation.\n"
    "ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. ë³¸ë¬¸ ë¬¸ì¥ì´ë‚˜ ë‹¨ë½ì„ ì˜ì–´ë¡œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "í•„ìš”í•œ í•µì‹¬ ê°œë…ë§Œ ê´„í˜¸ ì•ˆì— ì˜ì–´ë¡œ ì§§ê²Œ ë³‘ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
    "ëŒ€í™” ë‚´ìš©ê³¼ ëª…ì‹œì ìœ¼ë¡œ ì—°ê²°ë˜ì§€ ì•ŠëŠ” ìˆ«ì, ì—°ë„, ì‹œì¥ ê·œëª¨, êµ¬ì²´ ì‚¬ë¡€ëŠ” ì„ì˜ë¡œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "ëŒ€í™”ì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´, 'ëŒ€í™” ë‚´ì—­ë§Œìœ¼ë¡œëŠ” ì¶©ë¶„í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.'ë¼ëŠ” ë¬¸ì¥ì„ í¬í•¨í•´ í•œê³„ì ì„ ëª…ì‹œí•˜ì„¸ìš”."
)


def answer_question(question: str) -> str:
    if not question.strip():
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    _ = get_vectorstore()

    docs = hybrid_search(
        question,
        semantic_k=30,
        keyword_k=30,
        combined_k=12,
    )

    context = format_docs(docs[:8])
    messages = [
        {
            "role": "system",
            "content": QA_SYSTEM_PROMPT,
        },
        {
            "role": "user",
            "content": f"ì§ˆë¬¸: {question}\n\nì°¸ê³  ë¬¸ì„œ:\n{context}",
        },
    ]
    return _call_ollama_chat(messages)


def generate_conversation_report(history: List[Dict[str, str]]) -> str:
    if not history:
        raise HTTPException(
            status_code=400, detail="ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    lines = []
    for msg in history:
        role = msg.get("role")
        content = msg.get("content", "")
        if not content:
            continue
        role_label = "ì‚¬ìš©ì" if role == "user" else "AI"
        lines.append(f"{role_label}: {content}")

    conversation_text = "\n".join(lines)

    _ = get_vectorstore()

    messages = [
        {
            "role": "system",
            "content": REPORT_SYSTEM_PROMPT,
        },
        {
            "role": "user",
            "content": (
                "ë‹¤ìŒì€ ì‚¬ìš©ì(íŒ¨ì…˜ MD)ì™€ AI ë¦¬ì„œì¹˜ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ëŒ€í™” ë¡œê·¸ì…ë‹ˆë‹¤.\n"
                "ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
                f"ëŒ€í™” ë¡œê·¸:\n{conversation_text}\n\n"
                "ğŸ“Œ ë¦¬í¬íŠ¸ êµ¬ì„±ì€ ë‹¤ìŒ ì„¹ì…˜ì„ í¬í•¨í•´ ì£¼ì„¸ìš”.\n"
                "1. Executive Summary\n"
                "2. Key Insights (bullet í˜•íƒœ)\n"
                "3. Implications & Action Ideas (í˜„ì—… í™œìš© ì•„ì´ë””ì–´ ì¤‘ì‹¬)\n\n"
                "âš ï¸ ì£¼ì˜ì‚¬í•­\n"
                "- ë°˜ë“œì‹œ ëŒ€í™” ë‚´ìš©ì—ì„œ íŒŒìƒë  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë§Œ ì •ë¦¬í•  ê²ƒ\n"
                "- McKinsey/BoF ë¦¬í¬íŠ¸ì— ì¼ë°˜ì ìœ¼ë¡œ ë“±ì¥í•  ë²•í•œ ë¬¸ì¥ì´ë¼ë„, ëŒ€í™”ì— ì „í˜€ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤ë©´ ìƒì„±í•˜ì§€ ë§ ê²ƒ\n"
                "- í•œêµ­ì–´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ë˜, í•„ìš”í•œ í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°\n"
                "- ë¬¸ì¥ì€ ì§§ê³  ëª…ë£Œí•˜ê²Œ, ì‹¤ì œ ë³´ê³ ì„œì— ë°”ë¡œ ë¶™ì—¬ ë„£ì„ ìˆ˜ ìˆëŠ” í†¤ìœ¼ë¡œ ì‘ì„±"
            ),
        },
    ]

    return _call_ollama_chat(messages)
