import os
from collections import defaultdict
from typing import List, Dict, Any

from dotenv import load_dotenv
from fastapi import HTTPException

from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings


load_dotenv()
_groq_key = os.getenv("GROQ_API_KEY")


def _ensure_groq_key():
    if not _groq_key:
        raise HTTPException(
            status_code=500,
            detail="GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ .envì— GROQ_API_KEYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.",
        )


_vectorstore: FAISS | None = None
_bm25_retriever: BM25Retriever | None = None
_llm: ChatGroq | None = None
_by_year_chapter: Dict[Any, Any] | None = None
_by_chapter: Dict[Any, Any] | None = None

CHAPTER_LABELS = ["Global Economy", "Consumer Shifts", "Fashion System"]


def get_vectorstore() -> FAISS:
    global _vectorstore
    if _vectorstore is not None:
        return _vectorstore

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    base_dir = os.path.dirname(__file__)
    faiss_dir = os.path.join(base_dir, "faiss_index")

    if not os.path.isdir(faiss_dir):
        raise HTTPException(
            status_code=500,
            detail="faiss_index ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ktb_web4/faiss_index ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
        )

    _vectorstore = FAISS.load_local(
        faiss_dir, embeddings, allow_dangerous_deserialization=True
    )
    return _vectorstore


def get_llm() -> ChatGroq:
    global _llm
    if _llm is not None:
        return _llm

    _ensure_groq_key()
    _llm = ChatGroq(
        model_name="llama-3.1-8b-instant",
        temperature=0.1,
        groq_api_key=_groq_key,
    )
    return _llm


def get_bm25_retriever() -> BM25Retriever:
    global _bm25_retriever
    if _bm25_retriever is not None:
      return _bm25_retriever

    vs = get_vectorstore()
    all_docs = list(vs.docstore._dict.values())
    _bm25_retriever = BM25Retriever.from_documents(all_docs, k=50)
    return _bm25_retriever


def get_grouped_docs():
    global _by_year_chapter, _by_chapter
    if _by_year_chapter is not None and _by_chapter is not None:
        return _by_year_chapter, _by_chapter

    vs = get_vectorstore()
    all_docs = list(vs.docstore._dict.values())

    by_year_chapter = defaultdict(list)
    by_chapter = defaultdict(list)

    for d in all_docs:
        year = d.metadata.get("year")
        chapter = d.metadata.get("chapter")
        by_year_chapter[(year, chapter)].append(d)
        by_chapter[chapter].append(d)

    _by_year_chapter, _by_chapter = by_year_chapter, by_chapter
    return _by_year_chapter, _by_chapter


def hybrid_search(
    query: str,
    semantic_k: int = 30,
    keyword_k: int = 30,
    combined_k: int = 12,
    chapter_filter: str | None = None,
    region_filter: str | None = None,
):
    vs = get_vectorstore()
    bm25 = get_bm25_retriever()

    semantic_docs = vs.similarity_search(query, k=semantic_k)
    keyword_docs = bm25.invoke(query)[:keyword_k]

    def make_key(doc):
        return (
            doc.metadata.get("source"),
            doc.metadata.get("page"),
            doc.page_content,
        )

    scores: dict = {}
    n_sem = len(semantic_docs) or 1
    n_kw = len(keyword_docs) or 1

    for rank, doc in enumerate(semantic_docs):
        key = make_key(doc)
        sem_score = (n_sem - rank) / n_sem
        prev_sem, prev_kw, prev_doc = scores.get(key, (0.0, 0.0, doc))
        scores[key] = (max(prev_sem, sem_score), prev_kw, doc)

    for rank, doc in enumerate(keyword_docs):
        key = make_key(doc)
        kw_score = (n_kw - rank) / n_kw
        prev_sem, prev_kw, prev_doc = scores.get(key, (0.0, 0.0, doc))
        scores[key] = (prev_sem, max(prev_kw, kw_score), doc)

    alpha = 0.6
    scored_docs = []
    for sem_score, kw_score, doc in scores.values():
        final_score = alpha * sem_score + (1 - alpha) * kw_score

        if chapter_filter and doc.metadata.get("chapter") != chapter_filter:
            continue
        if region_filter and doc.metadata.get("region") != region_filter:
            continue

        scored_docs.append((final_score, doc))

    if not scored_docs:
        scored_docs = [
            (alpha * ((n_sem - i) / n_sem), d) for i, d in enumerate(semantic_docs)
        ]

    scored_docs.sort(key=lambda x: x[0], reverse=True)
    return [d for _, d in scored_docs[:combined_k]]


def format_docs(docs) -> str:
    processed = []
    for d in docs:
        src = os.path.basename(d.metadata.get("source", ""))
        page = d.metadata.get("page", "?")
        year = d.metadata.get("year", "")
        chapter = d.metadata.get("chapter", "")
        region = d.metadata.get("region", "")
        if region:
            header = f"[{year} / {chapter} / {region} / {src} p.{page}]"
        else:
            header = f"[{year} / {chapter} / {src} p.{page}]"
        processed.append(header + "\n" + d.page_content)
    return "\n\n".join(processed)


qa_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a professional Fashion MD Research Assistant.\n"
            "Use ONLY the content from McKinsey & BoF 'State of Fashion' (2021â€“2025).\n"
            "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, í•µì‹¬ ìš©ì–´ëŠ” ì˜ì–´ ë³‘ê¸°í•´ì¤˜.",
        ),
        (
            "human",
            "ì§ˆë¬¸: {question}\n\n"
            "ì°¸ê³  ë¬¸ì„œ:\n{context}",
        ),
    ]
)

report_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a senior fashion strategy consultant.\n"
            "Below is a conversation between a Fashion MD and an AI research assistant\n"
            "about insights from McKinsey & BoF 'State of Fashion' (2021â€“2025).\n"
            "Use ONLY information that can be reasonably grounded in this conversation.\n"
            "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , í•µì‹¬ ê°œë…ì€ í•„ìš”í•  ë•Œë§Œ ì˜ì–´ ë³‘ê¸°í•´ì¤˜.",
        ),
        (
            "human",
            "ë‹¤ìŒì€ ì‚¬ìš©ì(íŒ¨ì…˜ MD)ì™€ AI ë¦¬ì„œì¹˜ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ëŒ€í™” ë¡œê·¸ì…ë‹ˆë‹¤.\n"
            "ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
            "ëŒ€í™” ë¡œê·¸:\n{conversation}\n\n"
            "ğŸ“Œ ë¦¬í¬íŠ¸ êµ¬ì„±ì€ ë‹¤ìŒ ì„¹ì…˜ì„ í¬í•¨í•´ ì£¼ì„¸ìš”.\n"
            "1. Executive Summary\n"
            "2. Key Insights (bullet í˜•íƒœ)\n"
            "3. Implications & Action Ideas (í˜„ì—… í™œìš© ì•„ì´ë””ì–´ ì¤‘ì‹¬)\n\n"
            "âš ï¸ ì£¼ì˜ì‚¬í•­\n"
            "- ë°˜ë“œì‹œ ëŒ€í™” ë‚´ìš©ì—ì„œ íŒŒìƒë  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë§Œ ì •ë¦¬í•  ê²ƒ\n"
            "- McKinsey/BoF ë¦¬í¬íŠ¸ì— ì¼ë°˜ì ìœ¼ë¡œ ë“±ì¥í•  ë²•í•œ ë¬¸ì¥ì´ë¼ë„, ëŒ€í™”ì— ì „í˜€ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤ë©´ ìƒì„±í•˜ì§€ ë§ ê²ƒ\n"
            "- í•œêµ­ì–´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ë˜, í•„ìš”í•œ í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°\n"
            "- ë¬¸ì¥ì€ ì§§ê³  ëª…ë£Œí•˜ê²Œ, ì‹¤ì œ ë³´ê³ ì„œì— ë°”ë¡œ ë¶™ì—¬ ë„£ì„ ìˆ˜ ìˆëŠ” í†¤ìœ¼ë¡œ ì‘ì„±",
        ),
    ]
)


def answer_question(question: str) -> str:
    if not question.strip():
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    _ = get_vectorstore()
    _ = get_llm()

    docs = hybrid_search(
        question,
        semantic_k=30,
        keyword_k=30,
        combined_k=12,
    )

    context = format_docs(docs[:8])
    chain = qa_prompt | get_llm() | StrOutputParser()
    return chain.invoke({"question": question, "context": context})


def generate_conversation_report(history: List[Dict[str, str]]) -> str:
    if not history:
        raise HTTPException(
            status_code=400, detail="ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    lines = []
    for msg in history:
        role = msg.get("role")
        content = msg.get("content", "")
        if not content:
            continue
        role_label = "ì‚¬ìš©ì" if role == "user" else "AI"
        lines.append(f"{role_label}: {content}")

    conversation_text = "\n".join(lines)

    _ = get_vectorstore()
    _ = get_llm()

    chain = report_prompt | get_llm() | StrOutputParser()
    return chain.invoke({"conversation": conversation_text})

